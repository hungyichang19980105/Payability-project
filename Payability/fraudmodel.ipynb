{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import lightgbm as lgbm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_curve, roc_auc_score, accuracy_score, recall_score, confusion_matrix, precision_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset\n",
    "df = pd.read_csv('data/input.csv')\n",
    "df2 = df[['transaction_risk_score','cc_amount','ledger_balance','cardholder_presence','card_presence',\n",
    "          'partial_approval_capable','channel','processing_type','date','cc_acceptor_state','cc_acceptor_country','is_fraud']]\n",
    "df2 = df2.sort_values(by='date')\n",
    "df2 = df2.drop(\"date\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Split data\n",
    "def GeneralSplit(data, ratio, method):\n",
    "    training_set, test_set = np.split(data, [int(ratio *len(data))])\n",
    "\n",
    "    X_train = training_set.drop(\"is_fraud\",axis=1)\n",
    "    y_train = training_set[['is_fraud']].values.flatten()\n",
    "\n",
    "    X_test = test_set.drop(\"is_fraud\",axis=1)\n",
    "    y_test = test_set[['is_fraud']].values.flatten()\n",
    "\n",
    "    scalar = StandardScaler()\n",
    "    x_train_scale = scalar.fit_transform(X_train)\n",
    "    x_test_scale = scalar.transform(X_test)\n",
    "    \n",
    "    if method == \"RandomForest\":\n",
    "        clf = RandomForestClassifier(n_estimators=500, max_depth=15, random_state=0)  \n",
    "    elif method == \"LightGBM\":\n",
    "        clf = lgbm.LGBMClassifier(objective=\"binary\", n_estimators=10000)\n",
    "    \n",
    "    print('-----' + method + '-----')\n",
    "    print('Size of train set: ', X_train.shape)\n",
    "    print('Size of test set: ', X_test.shape)\n",
    "    print()\n",
    "    \n",
    "    clf.fit(x_train_scale,y_train)\n",
    "    pred = clf.predict(x_test_scale)\n",
    "    \n",
    "    precisions = round(precision_score(y_test, pred),3)\n",
    "    recalls = round(recall_score(y_test, pred),3)\n",
    "    accuracies = round(accuracy_score(y_test, pred),3)\n",
    "    f1_scores = round(f1_score(y_test, pred),3)\n",
    "    \n",
    "    fpr, tpr, thresh = roc_curve(y_test, pred, pos_label=1)\n",
    "    random_probs = [0 for i in range(len(y_test))]\n",
    "    p_fpr, p_tpr, _ = roc_curve(y_test, random_probs, pos_label=1)\n",
    "    auc_scores = round(roc_auc_score(y_test, pred),3)\n",
    "    \n",
    "    plt.style.use('seaborn')\n",
    "    plt.plot(fpr, tpr, linestyle='--',color='orange', label=method)\n",
    "    plt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\n",
    "    plt.title('ROC curve')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive rate')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    \n",
    "    print('Precision: ', precisions)\n",
    "    print('Recall: ', recalls)\n",
    "    print('Accuracy: ', accuracies)\n",
    "    print('F1_score: ', f1_scores)\n",
    "    print('AUC: ' + str(auc_scores))\n",
    "    print('Confusion-matrix: \\n' + str(confusion_matrix(y_test, pred)) + '\\n')\n",
    "    return [precisions, recalls, accuracies, f1_scores, auc_scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series split\n",
    "\n",
    "def TimeSeriesKFold(X_train, y_train, number_folds, method):\n",
    "    print(\"-----\" + method + \"-----\")\n",
    "    print('Size of data set: ', X_train.shape)\n",
    "    k = int(np.floor(float(X_train.shape[0]) / number_folds))\n",
    "    print('Size of each fold: ', k)\n",
    "    accuracies = np.zeros(number_folds-1)\n",
    "    recalls = np.zeros(number_folds-1)\n",
    "    precisions = np.zeros(number_folds-1)\n",
    "    f1_scores = np.zeros(number_folds-1)\n",
    "    auc_scores = np.zeros(number_folds-1)\n",
    "    if method == \"RandomForest\":\n",
    "        clf = RandomForestClassifier(n_estimators=500, max_depth=15, random_state=0)  \n",
    "    elif method == \"LightGBM\":\n",
    "        clf = lgbm.LGBMClassifier(objective=\"binary\", n_estimators=10000)\n",
    "    for i in range(2, number_folds + 1):\n",
    "        print()\n",
    "        split = float(i-1)/i\n",
    "        print('Splitting the first ' + str(i) + ' chunks with ratio ' + str(i-1) + ':1')\n",
    "        X = X_train[:(k*i)]\n",
    "        y = y_train[:(k*i)]\n",
    "        print('Size of train + test: ', X.shape)\n",
    "        index = int(np.floor(X.shape[0] * split))\n",
    "        X_trainFolds = X[:index]        \n",
    "        y_trainFolds = y[:index]\n",
    "        \n",
    "        # fold used to test the model\n",
    "        X_testFold = X[(index + 1):]\n",
    "        y_testFold = y[(index + 1):]\n",
    "            \n",
    "        clf.fit(X_trainFolds,y_trainFolds)\n",
    "        pred = clf.predict(X_testFold)\n",
    "        precisions[i-2] = round(precision_score(y_testFold, pred),3)\n",
    "        recalls[i-2] = round(recall_score(y_testFold, pred),3)\n",
    "        accuracies[i-2] = round(accuracy_score(y_testFold, pred),3)\n",
    "        f1_scores[i-2] = round(f1_score(y_testFold, pred),3)\n",
    "        \n",
    "        fpr, tpr, thresh = roc_curve(y_testFold, pred, pos_label=1)\n",
    "        random_probs = [0 for i in range(len(y_testFold))]\n",
    "        p_fpr, p_tpr, _ = roc_curve(y_testFold, random_probs, pos_label=1)\n",
    "        auc_scores[i-2] = round(roc_auc_score(y_testFold, pred),3)\n",
    "        \n",
    "        print('Precision on fold ' + str(i) + ': ', precisions[i-2])\n",
    "        print('Recall on fold ' + str(i) + ': ', recalls[i-2])\n",
    "        print('Accuracy on fold ' + str(i) + ': ', accuracies[i-2])\n",
    "        print('F1_score on fold ' + str(i) + ': ', f1_scores[i-2])\n",
    "        print('AUC on fold ' + str(i) + ': ', auc_scores[i-2])\n",
    "        print('Confusion-matrix: \\n' + str(confusion_matrix(y_testFold, pred)))\n",
    "        \n",
    "        plt.style.use('seaborn')\n",
    "        plt.plot(fpr, tpr, linestyle='--',color='orange', label=method)\n",
    "        plt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\n",
    "        plt.title('ROC curve')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive rate')\n",
    "        plt.legend(loc='best')\n",
    "        plt.show()\n",
    "        \n",
    "    print('Precision mean: ' + str(precisions.mean()))\n",
    "    print('Recall mean: ' + str(recalls.mean()))\n",
    "    print('Accuracy mean: ' + str(accuracies.mean()))\n",
    "    print('F1-Score mean: ' + str(f1_scores.mean()))\n",
    "    print('AUC mean: ' + str(auc_scores.mean()))\n",
    "    return [precisions, recalls, accuracies, f1_scores, auc_scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
